
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Low-Hanging Fruit</title>
  <meta name="author" content="Jeremy Velev">

  
  <meta name="description" content="I don&rsquo;t have anything in mind to post this week, so I&rsquo;m reposting this article I wrote for myself in 2010 after deep-diving into Bayesian &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://jmckib.github.io">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="Low-Hanging Fruit" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="./javascripts/lib/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">Low-Hanging Fruit</a></h1>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:jmckib.github.io" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/12/01/designing-an-artificial-intelligence-an-introduction-to-bayesian-statistics/">Designing an Artificial Intelligence: An Introduction to Bayesian Statistics</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-12-01T21:59:00-05:00" pubdate data-updated="true">Dec 1<span>st</span>, 2013</time>
        
           | <a href="/blog/2013/12/01/designing-an-artificial-intelligence-an-introduction-to-bayesian-statistics/#disqus_thread"
             data-disqus-identifier="http://jmckib.github.io/blog/2013/12/01/designing-an-artificial-intelligence-an-introduction-to-bayesian-statistics/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><blockquote><p>I don&rsquo;t have anything in mind to post this week, so I&rsquo;m reposting this article I wrote for myself in 2010 after deep-diving into Bayesian statistics and finally making sense of it (at least in my own mind). I hope it makes sense to you too!</p></blockquote>

<h3>
First principles
</h3>




<p>Drawing conclusions is relatively straightforward when you&#8217;re sure of what
you know.  <em>A</em> is true, and <em>A</em> implies <em>B</em>, therefore <em>B</em> is true.  Here <em>A</em> and <em>B</em> are
<strong>propositions</strong>, statements which are (in principle) either true or false,
even if we don&#8217;t know which of the two they are.
</p>




<p>However, most decisions that we need to make depend
on propositions that can&#8217;t be judged true or false given the data at
hand.  Is it going to rain today?  Will I get sick if I don&#8217;t get a flu
vaccine?  Will the stock market go up or down in six months? If we knew the
answers to these questions ahead of time, even approximately, it would
allow us to make better decisions.
</p>




<p>Is it possible then, to design an artificial intelligence (an AI) to help us
reason about uncertain propositions in an optimal way?  In order
to begin, we would first need to come up with a relatively satisfying
definition of &#8220;optimal&#8221;.  At first one might hope for an AI that
that provides some sort of guarantee of the correctness of the answers it provides.  As far
as I know this hasn&#8217;t been done (after all, we are dealing
with <em>uncertain</em> propositions), but there has been progress towards
satisfying the following four design goals:
</p>




<ol>
<li>
<strong>Universality</strong>: we would like our AI to be able to provide answers to a
wide variety of questions.
</li>

<li>
<strong>Simplicity</strong>: as a practical matter, a simple design is easier to understand and implement.
</li>

<li>
<strong>Agrees with our intuition</strong>: I can&#8217;t be more precise about this until I get
into the details of our AI&#8217;s internals, but don&#8217;t worry, I will be very
specific when I invoke this requirement.
</li>

<li>
<strong>Consistency</strong>: we would like for our AI to give us the same answer
every time we ask it a specific question.
</li>
</ol>




<p>
Of the four of these criteria, the last one, consistency, is perhaps the
least important.  If our AI isn&#8217;t universal it won&#8217;t be able to provide
an answer to many of our questions; if isn&#8217;t simple it also won&#8217;t be
able to provide many answers, because it will be too difficult to implement
and use; and if it doesn&#8217;t agree with our intuition then the
answers it does give us will appear absurd or arbitrary.  However, a lack of
consistency in the answers it provides doesn&#8217;t result in any similar
disadvantage.
</p>




<p>
This may seem strange, because normally if we are designing a machine to
evaluate propositions then a lack of consistency is an indication of a
serious error.  For example, if you had a calculator that gave a different
answer every time you typed in &#8220;2308 * 5982&#8221; then (even not knowing the
answer yourself) you would be able to tell there is something wrong with the
calculator, since there is only one number equal to 2308 * 5982, and it can
be determined based on the given data.
</p>




<p>
However, imagine that the purpose of your calculator was to determine the
mass of Saturn based on a series of astronomical observations. In that case,
we can&#8217;t say that something is wrong with the calculator because it doesn&#8217;t
output the same answer every time we input the same exact data, since
(in the context of this example) the data given to the calculator is not sufficient to
determine the actual mass of Saturn.
</p>




<p>
However, there is at least one practical reason to desire consistency;
debugging our AI will be easier when we know exactly what response to expect
from it based on a given input.  So although I will not stress the
importance of consistency, I will mention in passing how we can ensure that
our AI will provide consistent answers, at least for debugging purposes.
</p>




<h3>
AI internals
</h3>




<p>When human beings reason about uncertain propositions, we tend to attach
weights or &#8220;levels of belief&#8221; to the different propositions under
consideration.  For example, if there are clouds in the sky one might think
of the proposition &#8220;it is going to rain today&#8221; as having more weight than
the negation of that statement, &#8220;it is not going to rain today&#8221;.
</p>




<p>
This suggests one way to design our AI. Given an uncertain proposition <em>A</em>
which the AI must decide on and some background information <em>I</em> related to <em>A</em>,
the AI could assign a real number to <em>A</em>, called <em>B(A | I)</em>, which you can
pronounce &#8220;the AI&#8217;s belief in proposition <em>A</em> given background information <em>I</em>&#8221;.
The AI could also assign a real number to the negation of <em>A, &minus;A</em>, which we
would call <em>B(&minus;A | I)</em>.
</p>




<p>
If <em>B(A | I) ≥ B(&minus;A | I)</em>, then the AI&#8217;s output will be &#8221;<em>A</em> is true&#8221;, and
otherwise &#8221;<em>A</em> is false&#8221;.  As you can see, the AI displays a preference for
the original proposition <em>A</em> when the beliefs are equal. We could just as
easily have had things the other way, but since the AI must make some
decision we might as well choose one arbitrarily (here you can see the
consistency issue already beginning to rear its head).
</p>




<p>
It is important to understand the following point: the AI&#8217;s level of belief
<em>B(A | I)</em> in the proposition <em>A</em> is only a description of the AI&#8217;s internal
state; it has no significance beyond the internals of our AI.  It is not
a statement about the true nature of the proposition <em>A</em>, nor is it a statement about the long
term frequency of <em>A</em> being true in repeated trials, whatever that would mean.
It is merely a description of the AI&#8217;s internals; any attempt to attach
additional significance to it will only result in confusion.
</p>




<p>
It is also worth mentioning that this is not the only way to design such an
AI; instead of assigning one real number to a given proposition we could
assign several, or we could imagine some other mechanism which doesn&#8217;t
involve numbers at all.  This idea merely suggests itself to us because it
is simple, and it is similar to the way we humans think.
</p>




<p>
The only question now is, how will our AI determine <em>B(A | I)</em> in a way that
satisfies our original four design goals?  To do this, I will state just six
rules that our AI will follow as it assigns belief values to propositions.
Each rule will be justified by one or more of our design goals, and all of
the rules together will be enough to fully determine how our AI will form
beliefs.  We are already familiar with the first rule:
</p>




<p>
D1) Given a proposition <em>A</em> and some background information <em>I</em>, the AI will assign a real number
B(A | I) to <em>A</em>.  This is similar to the way that humans reason about uncertain
propositions.
</p>




<p>
D2) There exists some real number <em>T</em> such that <em>B(A | I) &le; T</em> for every <em>A</em> and
I.  If <em>A</em> is known to be true, then the AI should assign <em>B(A | I) = T</em>.
</p>




<p>
Logically this is not essential, and in fact it doesn&#8217;t follow from any of
our design goals.  The reason for this rule is merely practical: because of storage constraints we cannot actually create an AI
with unbounded belief values.
</p>




<p>
D3) The assignment of beliefs is compatible with everyday logic.
</p>




<p>
This stems from our third design goal; the AI must assign beliefs to propositions in a way that
agrees with our intuition. For example, if <em>A</em> and A&#8217; are equivalent
propositions then the AI should assign equal levels of belief to <em>A</em> and <em>A&#8217;</em>,
so that <em>B(A | I) = B(A&#8217; | I)</em>.  There are some other requirements here, but
they are all similarly straightforward (see <a href="http://ksvanhorn.com/bayes/Papers/rcox.pdf">Kevin van Horn</a> for details).
</p>




<p>
D4) There is a nonincreasing function <em>S</em> such that <em>S(B(A | I)) = B(&minus;A | I)</em>.
Define <em>F = S(T)</em>.
</p>




<p>
This also is meant to agree with our intuition.  In simple English, it means
that if some new data causes the AI to assign a higher level of belief to A,
then it&#8217;s level of belief in the negation of <em>A</em> should not increase as
a result.  We could require that the AI&#8217;s belief in <em>-A</em> should
actually decrease rather than just not increase, but we are trying to be as
unrestrictive as possible while still satisfying our original design goals.
</p>




<p>
D5) There are no intervals contained in <em>(F, T)</em> that contain belief values
which are not possible for our AI to have (the required statement is
actually somewhat stronger than
this. See <a href="http://ksvanhorn.com/bayes/Papers/rcox.pdf">van Horn</a> for the details).
</p>




<p>
The primary justification for this requirement is that it appeals to our
intuition, since we humans do not think of levels of belief as being finite
and discrete, we think of them as being continuous.  If we reject D5 then we
are asserting that there is no &#8220;middle ground&#8221; between some two levels of belief
in a proposition&#8211;an awkward assertion in light of everyday experience.
</p>




<p>
D6) There is a strictly increasing, continuous function <em>F</em> such that
<em>B(A, C | I) = F(B(A | C, I) · B(C | I))</em> for all propositions <em>A</em> and <em>C</em> with
background information <em>I</em>. Here <em>B(A, C | I)</em> is a real number
that represents &#8220;the
AI&#8217;s belief in both <em>A</em> and <em>C</em>, given I&#8221;.
</p>




<p>
Once again, the argument for this requirement is that it appeals to our
common sense.  One&#8217;s belief in a both <em>A</em> and <em>C</em> should depend on both one&#8217;s
belief in C, and one&#8217;s belief in <em>A</em> given that <em>C</em> is true.  See
<a href="http://omega.math.albany.edu:8008/JaynesBook.html">Jaynes</a> or
<a href="http://ksvanhorn.com/bayes/Papers/rcox.pdf">van Horn</a> for
further arguments in favor of this requirement.
</p>




<h3>Bayes&#8217; Rule</h3>




<p>
That&#8217;s it, we have set out six rules for how our AI should operate
internally, and that is enough to completely determine the algorithm that
our AI will use to calculate belief values for nearly every kind of
proposition it could encounter. For more information on this result, known
as Cox&#8217;s
Theorem, <a href="http://omega.math.albany.edu:8008/JaynesBook.html">van
    Horn&#8217;s paper</a> is the best resource I&#8217;ve found.
</p>




<p>
Without going into the proof, it turns out that that the function B can be
rescaled so that <em>B(T) = 1</em> and <em>B(F) = 0</em>, which agrees with the usual mapping of 1 to
true propositions, and 0 to false propositions.  It can also be shown that <em>B</em>
can be formulated so that the following two rules hold:
</p>




<p>
<strong>The sum rule</strong>:
<em>B(&minus;A | I) = 1 &minus; B(A | I)</em>
</p>




<p>
<strong>The product rule</strong>:
<em>B(A, C | I) = B(A | C, I) · B(C | I)</em>
</p>




<p>
Now if we switch the roles of <em>A</em> and <em>C</em> in the product rule, we have
</p>




<p>
<em>B(C, A | I) = B(C | A, I) · B(A | I)</em>
</p>




<p>
but <em>B(C, A | I) = B(A, C | I)</em> by D1, since (<em>A and C</em>) and (<em>C
and A</em>) are equivalent propositions, so we have that
</p>




<p>
<em>B(A | C, I) · B(C | I) = B(C | A, I) · B(A | I)</em>
</p>




<p>
Finally, by dividing both sides by <em>B(C | I)</em>, we have the Bayes&#8217;-Laplace Rule
(traditionally shortened to Bayes&#8217; Rule):
</p>




<p>
<strong>Bayes&#8217; Rule</strong>:
<center>
<img class="tex" alt="B(A \,|\, C,\, I) = {B(C\, |\, A,\, I) \cdot B(A \,|\, I)\over B(C\, |\, I)}" src="/images/2013-12-01/latex-image-1.png">
</center>
</p>




<p>
If we let <em>A</em> be our hypothesis, and <em>C</em> be the data we have collected
concerning the hypothesis, then we can see the true power of Bayes&#8217; rule.
<center>
<img class="tex" alt="B(hypothesis\, |\, data,\, I)\, =\, {B(data\, | \,hypothesis,\, I)\,\cdot\, B(hypothesis \,|\, I) \over B(data \,|\, I)}" src="/images/2013-12-01/latex-image-2.png">
</center>
</p>




<p>
The term <em>B(hypothesis | data, I)</em> is traditionally called the <strong>posterior</strong>,
since it describes our AI&#8217;s belief in the hypothesis <em>after</em> viewing the
data.  The term <em>B(data | hypothesis, I)</em> is called the <strong>evidence</strong> or more
traditionally the <strong>likelihood</strong>; it assigns a number to the degree to which the
hypothesis is supported by the data.  <em>B(hypothesis | I)</em> is the <strong>prior</strong>; it
shows our AI&#8217;s belief in the hypothesis <em>before</em> viewing the data.  <em>B(data |
I)</em> is simply a constant which does not depend on the hypothesis; it is
often ignored since we are usually only interested in choosing a good hypothesis.
</p>




<p>
One of the reasons that Bayes&#8217; Rule is so celebrated is because it precisely
describes the scientific method&#8211;the idea that we must use data to update
our beliefs about a hypothesis. From this simple equation our AI gains an
almost unlimited ability to learn and solve problems, because for almost every
problem we encounter, the process of finding a solution can be described as
&#8220;updating a hypothesis in light of the data&#8221;.
</p>




<p>
It is worth reiterating an earlier point; there is no claim here that this
is the only way to reason about uncertain propositions, or even the best
way. It is merely a widely applicable, simple, and intuitively appealing
way for humans (and machines designed by humans) to evaluate uncertain propositions.  While
Bayes&#8217; Rule is elegant and highly appealing in its own right, it
could not be described as a universal law in any meaningful way; it is just
a highly idealized description of the way humans reason about
uncertain propositions.
</p>




<h3>A last word about consistency and correctness</h3>




<p>When our AI encounters new data it will update its beliefs using Bayes&#8217;
  Rule, but what sort of beliefs should our AI have before encountering any
  data at all?  In other words, how is our AI supposed to select a
  prior?</p>




<p>
  A common approach is to select prior beliefs in a way
  that expresses total ignorance (though what this means has been the subject of
  some debate).  To that end, several guidelines have
  been developed for selecting ignorant priors, such as the
  the <a href="http://en.wikipedia.org/wiki/Principle_of_indifference">Principle of Indifference</a> and the <a href="http://en.wikipedia.org/wiki/Principle_of_maximum_entropy">Principle of Maximum Entropy</a>.
</p>




<p>
 I would argue that the main advantage of these principles is that they keep our AI consistent, so that a pair of AI who have seen the
  same data will assign the same belief values (a useful feature for
  debugging purposes).  However, I haven&#8217;t encountered an argument
  for why such guidelines give any real advantage in practice; we
  can&#8217;t argue that the AI&#8217;s belief assignments are more correct for
  having chosen a prior in one way rather than another.  This is part of
  a larger problem; there is no guarantee that our AI produces answers
  which reflect the real world at all.</p>




<p>What I just said may sound overly cynical, but I am only claiming that there has not <em>yet</em>
  been a proof that our  AI&#8217;s beliefs reflect the real world in
  some way.  I am not claiming that there will <em>never</em> be such a proof.  A good argument
  in favor of our third design goal (intuitiveness) is that an AI
  which agrees with intuition stands a good chance of having some
  claim to correctness, since our intuitions have evolved along with us over millions of
  years of making decisions under uncertainty.</p>




<p>If you would like to learn more, rather than overwhelm you with a long list of additional reading
  material I will just recommend one short book, <a href="http://www.amazon.com/Data-Analysis-Bayesian-Devinderjit-Sivia/dp/0198568320/ref=sr_1_1?s=books&ie=UTF8&qid=1290438345&sr=1-1">Data Analysis: A Bayesian Tutorial</a>.  It is
  full of good examples of Bayes&#8217; Rule being put to use in real
  problems, and after reading it you should be able to understand most books
  and papers on Bayesian&#8217; methods, allowing you to continue to learn on your own.
</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/11/24/the-easterlin-hypothesis/">The Easterlin Hypothesis</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-11-24T18:40:00-05:00" pubdate data-updated="true">Nov 24<span>th</span>, 2013</time>
        
           | <a href="/blog/2013/11/24/the-easterlin-hypothesis/#disqus_thread"
             data-disqus-identifier="http://jmckib.github.io/blog/2013/11/24/the-easterlin-hypothesis/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>In my <a href="/blog/2013/11/12/stumbling-on-happiness">last post</a>, I mentioned that most economists and psychologists agree that after people make it out of poverty and into the middle class, more money does not buy more happiness. However, my friend <a href="https://github.com/jianli">Jian Li</a> pointed out that this isn&rsquo;t entirely true, citing <a href="http://www.nytimes.com/2008/04/16/business/16leonhardt.html">an article in the NYTimes</a> about the recent work of two economists, Betsey Stevenson and Justin Wolfers.</p>

<p>I can&rsquo;t access the paper linked in the NYTimes article (the page is timing out), but I was able to find an <a href="http://www.econtalk.org/archives/2013/06/stevenson_and_w.html">EconTalk podcast with Stevenson and Wolfers</a> where the findings are discussed in detail. In it, they argue that contrary to the famed <a href="http://en.wikipedia.org/wiki/Easterlin_paradox">Easterlin paradox</a>, which they relabel as the &ldquo;Easterlin hypothesis&rdquo;, absolute income <em>is strongly positively correlated</em> with happiness. And the data seems to back them up.</p>

<p>For the moment, though, I stand by my previous claim that after escaping from poverty, additional income does not have a large effect on happiness, and it seems that Stevenson and Wolfers would agree. In the podcast they make two points that support this view:</p>

<ol>
<li><p>While maintaining that there is a strong positive correlation between absolute income and happiness (.8, to be exact), they stress that happiness is proportional to the log of income, meaning that each time a person&rsquo;s income is doubled it will only add a fixed constant to their level of happiness. For example, doubling someone&rsquo;s income from $50,000 a year to $100,000 a year will produce the same increase in happiness as further doubling their income from $100,000 to $200,000.</p>

<p>So money does buy happiness, but after a certain point the effect of an extra dollar, or even an extra thousand dollars, becomes negligible. I&rsquo;m not sure where these thresholds are located, perhaps it varies significantly from person to person, but for me, personally, I feel that there isn&rsquo;t much extra happiness to be gained from additional income beyond the middle-class level.</p></li>
<li><p>While Stevenson and Wolfers are certain there&rsquo;s a positive correlation between income and happiness, that doesn&rsquo;t mean that more money will make us as happy as we think it will, or that there aren&rsquo;t other elements of life that have a much bigger impact on happiness. This agrees with what Daniel Gilbert said in <a href="http://www.amazon.com/Stumbling-Happiness-Daniel-Gilbert/dp/1400077427">Stumbling on Happiness</a> (see <a href="/blog/2013/11/12/stumbling-on-happiness">last post</a>), namely, that people tend to overestimate how much happiness they&rsquo;ll get from additional income.</p></li>
</ol>


<p>In conclusion, I would be surprised if there&rsquo;s a silver bullet on the road to achieving greater happiness, but I do hope that there is some low-hanging fruit along the way. However, at least for those of us that don&rsquo;t live in poverty, additional income is not it.</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2013/11/12/stumbling-on-happiness/">Stumbling on Happiness</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2013-11-12T22:14:00-05:00" pubdate data-updated="true">Nov 12<span>th</span>, 2013</time>
        
           | <a href="/blog/2013/11/12/stumbling-on-happiness/#disqus_thread"
             data-disqus-identifier="http://jmckib.github.io/blog/2013/11/12/stumbling-on-happiness/">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>I think all of us can agree that we want to be happy. However, I would wager that most people are not as happy as they want to be. Most of us are constantly striving for something more, and if we could just <em>get there</em> &mdash; if we could get a more interesting job, financial independence, live in the perfect place, etc. &mdash; then we&rsquo;d be happy. However, in my experience, the happiness that I get once I finally <em>get there</em> is short-lived, and then my average level of happiness quickly returns to its previous level.</p>

<p>At times I&rsquo;ve accepted this as inevitable. &ldquo;It&rsquo;s just the human condition&rdquo;, I would say to myself. I&rsquo;ve grown used to the cycle; I&rsquo;m filled with a burning desire for something more, I go after it with all my energy, pour my heart into it, finally attain it, and then the desire refocuses elsewhere, on another goal that is just over the horizon. By now (I&rsquo;m 25 years old), the situation has gotten so ridiculous and repetitive that I&rsquo;ve decided I must be missing something, and so I started looking around for answers.</p>

<p>One of the first resources I stumbled on was Daniel Gilbert&rsquo;s book, <a href="http://www.amazon.com/Stumbling-Happiness-Daniel-Gilbert/dp/1400077427">Stumbling on Happiness</a>. He&rsquo;s also got <a href="http://www.ted.com/talks/dan_gilbert_asks_why_are_we_happy.html">an excellent TED Talk</a> that summarizes a few ideas in the book. Since I just finished the book, I thought I&rsquo;d use this first post to summarize what I think are its most important lessons.</p>

<p><img class="right" src="http://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/M%C3%BCller-Lyer_illusion.svg/200px-M%C3%BCller-Lyer_illusion.svg.png" title="The Müller-Lyer illusion. Which line is longer?" ></p>

<p>First, our ability to imagine the future, our <em>prospection</em>, is as error-prone as our ability to recall the past. The problem is that we prospect constantly, without even realizing it, and we never learn to question the products of our prospection. As Gilbert shows in parts 3-5 of the book, we make predictable errors as we try to imagine the future, just as our eyes make predictable errors when viewing an optical illusion.</p>

<p>Second, we have a powerful &ldquo;psychological immune system&rdquo;, as Gilbert calls it, which returns us to a baseline level of happiness when life gets rough. However, we do a poor job of anticipating how powerful this immune system is, leading us to systematically make incorrect predictions about our future emotional states.</p>

<p>Third, our powers of retrospection are as flawed as our powers of prospection, making it difficult for us to learn from our mistakes. For example, we often look forward to going on vacation, and then remember the vacation fondly, even if we didn&rsquo;t actually enjoy ourselves most of the time we were on vacation. How are we going to choose a better vacation next time if we trust blindly in our prospections and retrospections?</p>

<p>So all of this is fairly interesting and depressing, but can we use this knowledge to make better decisions? Yes, there are a few practical lessons in the book:</p>

<ol>
<li><p><strong>Learn to anticipate the actions of your psychological immune system</strong>, because it&rsquo;s a very real and powerful thing. Specifically, have courage and blunder forward, even if you&rsquo;re not sure of yourself. If you fail spectacularly your psychological immune system will pick up the pieces and return you to your normal level of happiness in short order.</p>

<p>Additionally, as Gilbert points out in Chapter 9, people more often regret not taking an action rather than taking it, &ldquo;the most popular regrets include not going to college, not grasping profitable business opportunities, and not spending enough time with friends and family.&rdquo; As he explains, this is because our mind&rsquo;s immune system is more capable of rationalizing an action than an inaction, because we can always feel that we benefited by the experiences we gained, even if we failed in every other aspect.</p></li>
<li><p><strong>When making an important decision, you shouldn&rsquo;t blindly trust your imagination.</strong> In most cases it will fail to make accurate predictions about your future emotional states. Instead, you should get your answers from people who are currently experiencing the reality you are contemplating.</p>

<p>There are two excellent examples of this that apply to most people. The first is money. Economists and psychologists generally agree that after people make it out of poverty and into the middle class, more money does not buy more happiness. How many of us actually take this knowledge to heart?</p>

<p>The second example is having children. Research indicates that couples who have children are less happy than before they had children, and only return to their previous level of happiness after children leave the home. Even throughout the day, women are less happy while taking care of their children than they are while doing other routine activities, like eating or watching TV. Do most people even consider this research when deciding to have children?</p></li>
</ol>


<p><span class='pullquote-right' data-pullquote='we must learn to question the products of our imagination'>
Most of us grow up trusting our imagination, but the overarching lesson of this book (which is backed by considerable evidence, read it and see!) is that we must learn to question the products of our imagination in the same way that we question other sources of information. We must look for more reliable ways of predicting our future emotional states or we will make the same mistakes again and again. Unfortunately, this is not always practical or easy to do in every situation, but from now on I will be on the lookout for better ways to predict my future happiness.
</span></p>
</div>
  
  


    </article>
  
  <div class="pagination">
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2013/12/01/designing-an-artificial-intelligence-an-introduction-to-bayesian-statistics/">Designing an Artificial Intelligence: An Introduction to Bayesian Statistics</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/11/24/the-easterlin-hypothesis/">The Easterlin Hypothesis</a>
      </li>
    
      <li class="post">
        <a href="/blog/2013/11/12/stumbling-on-happiness/">Stumbling on Happiness</a>
      </li>
    
  </ul>
</section>





  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2013 - Jeremy Velev -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'lowhangingfruitblog';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = '//' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = '//platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
